{"cells":[{"cell_type":"markdown","source":["# Reading Data - JSON Files\n\n**Technical Accomplishments:**\n- Read data from:\n  * JSON without a Schema\n  * JSON with a Schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c150310d-50d8-474a-b5c5-6777c5ddc13f"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"001274ea-4fdb-46b3-ba5a-2fb6c10732e4"}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d318b9ba-3ef9-4ad2-8c7d-5cfcf5477578"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%run \"./Includes/Utility-Methods\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ae31cf5-0b8a-469d-becf-eba30313f5db"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from JSON w/ InferSchema\n\nReading in JSON isn't that much different than reading in CSV files.\n\nLet's start with taking a look at all the different options that go along with reading in JSON files."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d08bdfe-90e5-4dcf-a619-04966b39dca5"}}},{"cell_type":"markdown","source":["### JSON Lines\n\nMuch like the CSV reader, the JSON reader also assumes...\n* That there is one JSON object per line and...\n* That it's delineated by a new-line.\n\nThis format is referred to as **JSON Lines** or **newline-delimited JSON** \n\nMore information about this format can be found at <a href=\"http://jsonlines.org/\" target=\"_blank\">http://jsonlines.org</a>.\n\n** *Note:* ** *Spark 2.2 was released on July 11th 2016. With that comes File IO improvements for CSV & JSON, but more importantly, **Support for parsing multi-line JSON and CSV files**. You can read more about that (and other features in Spark 2.2) in the <a href=\"https://databricks.com/blog/2017/07/11/introducing-apache-spark-2-2.html\" target=\"_blank\">Databricks Blog</a>.*"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c78f975-50f7-45f7-99fe-13abf6f4a169"}}},{"cell_type":"markdown","source":["### The Data Source\n* For this exercise, we will be using the file called **snapshot-2016-05-26.json** (<a href=\"https://wikitech.wikimedia.org/wiki/Stream.wikimedia.org/rc\" target=\"_blank\">4 MB</a> file from Wikipedia).\n* The data represents a set of edits to Wikipedia articles captured in May of 2016.\n* It's located on the DBFS at **dbfs:/mnt/training/wikipedia/edits/snapshot-2016-05-26.json**\n* Like we did with the CSV file, we can use **&percnt;fs ls ...** to view the file on the DBFS."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2895628-1dbc-486d-ac45-22461d40f6ba"}}},{"cell_type":"code","source":["%fs ls dbfs:/mnt/training/wikipedia/edits/snapshot-2016-05-26.json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9e9a8a5-03c8-4f1a-8856-018c8ce4b930"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Like we did with the CSV file, we can use **&percnt;fs head ...** to peek at the first couple lines of the JSON file."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"be7d0109-c6ac-4aa6-9a78-00f509ffa041"}}},{"cell_type":"code","source":["%fs head dbfs:/mnt/training/wikipedia/edits/snapshot-2016-05-26.json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bab1f26-1e36-4fe9-b23c-fa97acc81dba"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Read The JSON File\n\nThe command to read in JSON looks very similar to that of CSV.\n\nIn addition to reading the JSON file, we will also print the resulting schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19fdef1b-832d-41bc-9de2-9e47d9b77aa5"}}},{"cell_type":"code","source":["jsonFile = \"dbfs:/mnt/training/wikipedia/edits/snapshot-2016-05-26.json\"\n\nwikiEditsDF = (spark.read           # The DataFrameReader\n    .option(\"inferSchema\", \"true\")  # Automatically infer data types & column names\n    .json(jsonFile)                 # Creates a DataFrame from JSON after reading in the file\n )\nwikiEditsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9782bb2e-dd5c-40b2-9619-d210477adff8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["With our DataFrame created, we can now take a peak at the data.\n\nBut to demonstrate a unique aspect of JSON data (or any data with embedded fields), we will first create a temporary view and then view the data via SQL:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8be58042-e54e-458e-801d-385c43f80422"}}},{"cell_type":"code","source":["# create a view called wiki_edits\nwikiEditsDF.createOrReplaceTempView(\"wiki_edits\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fb99785-2dfc-47e4-a4f1-5f40d0f32b1b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["And now we can take a peak at the data with simple SQL SELECT statement:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49a9ce19-b6d7-42a8-9bf7-495b1958c96a"}}},{"cell_type":"code","source":["%sql\n\nSELECT * FROM wiki_edits "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d6b12d5-8809-4db5-ad03-493267722b4d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Notice the **geocoding** column has embedded data.\n\nYou can expand the fields by clicking the right triangle in each row.\n\nBut we can also reference the sub-fields directly as we see in the following SQL statement:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e0e6839-4d8f-45e1-a2bd-c05d33395828"}}},{"cell_type":"code","source":["%sql\n\nSELECT channel, page, geocoding.city, geocoding.latitude, geocoding.longitude \nFROM wiki_edits \nWHERE geocoding.city IS NOT NULL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b157edc8-d588-42fa-87df-805c7e08442e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Review: Reading from JSON w/ InferSchema\n\nWhile there are similarities between reading in CSV & JSON there are some key differences:\n* We only need one job even when inferring the schema.\n* There is no header which is why there isn't a second job in this case - the column names are extracted from the JSON object's attributes.\n* Unlike CSV which reads in 100% of the data, the JSON reader only samples the data.  \n**Note:** In Spark 2.2 the behavior was changed to read in the entire JSON file."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc8237fc-bcc6-428a-b186-cb3a615fa8d7"}}},{"cell_type":"markdown","source":["##![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Reading from JSON w/ User-Defined Schema\n\nTo avoid the extra job, we can (just like we did with CSV) specify the schema for the `DataFrame`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed1b0794-641b-4c63-92df-6d8046df3ff0"}}},{"cell_type":"markdown","source":["### Step #1 - Create the Schema\n\nCompared to our CSV example, the structure of this data is a little more complex.\n\nNote that we can support complex data types as seen in the field `geocoding`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf0f157d-08d1-4c9b-b42a-e89b89ae3f01"}}},{"cell_type":"code","source":["# Required for StructField, StringType, IntegerType, etc.\nfrom pyspark.sql.types import *\n\njsonSchema = StructType([\n  StructField(\"channel\", StringType(), True),\n  StructField(\"comment\", StringType(), True),\n  StructField(\"delta\", IntegerType(), True),\n  StructField(\"flag\", StringType(), True),\n  StructField(\"geocoding\", StructType([\n    StructField(\"city\", StringType(), True),\n    StructField(\"country\", StringType(), True),\n    StructField(\"countryCode2\", StringType(), True),\n    StructField(\"countryCode3\", StringType(), True),\n    StructField(\"stateProvince\", StringType(), True),\n    StructField(\"latitude\", DoubleType(), True),\n    StructField(\"longitude\", DoubleType(), True)\n  ]), True),\n  StructField(\"isAnonymous\", BooleanType(), True),\n  StructField(\"isNewPage\", BooleanType(), True),\n  StructField(\"isRobot\", BooleanType(), True),\n  StructField(\"isUnpatrolled\", BooleanType(), True),\n  StructField(\"namespace\", StringType(), True),\n  StructField(\"page\", StringType(), True),\n  StructField(\"pageURL\", StringType(), True),\n  StructField(\"timestamp\", StringType(), True),\n  StructField(\"url\", StringType(), True),\n  StructField(\"user\", StringType(), True),\n  StructField(\"userURL\", StringType(), True),\n  StructField(\"wikipediaURL\", StringType(), True),\n  StructField(\"wikipedia\", StringType(), True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"151d7421-8392-49b3-91f1-82e06ddad80b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["That was a lot of typing to get our schema!\n\nFor a small file, manually creating the the schema may not be worth the effort.\n\nHowever, for a large file, the time to manually create the schema may be worth the trade off of a really long infer-schema process."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b714f498-c161-44db-b82d-5b71672fbfd1"}}},{"cell_type":"markdown","source":["### Step #2 - Read in the JSON\n\nNext, we will read in the JSON file and once again print its schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4bdcbc85-e0d1-41aa-ae77-1e105a5980c0"}}},{"cell_type":"code","source":["(spark.read            # The DataFrameReader\n  .schema(jsonSchema)  # Use the specified schema\n  .json(jsonFile)      # Creates a DataFrame from JSON after reading in the file\n  .printSchema()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06c2cad1-bb95-43ed-88ac-75d4f669437a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Review: Reading from JSON w/ User-Defined Schema\n* Just like CSV, providing the schema avoids the extra jobs.\n* The schema allows us to rename columns and specify alternate data types.\n* Can get arbitrarily complex in its structure."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f98768e-1492-4bf9-8364-2bf784b61ee9"}}},{"cell_type":"markdown","source":["Let's take a look at some of the other details of the `DataFrame` we just created for comparison sake."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"687118e5-d0e9-44d3-92d7-4828da2777ad"}}},{"cell_type":"code","source":["jsonDF = (spark.read\n  .schema(jsonSchema)\n  .json(jsonFile)    \n)\nprint(\"Partitions: \" + str(jsonDF.rdd.getNumPartitions()))\nprintRecordsPerPartition(jsonDF)\nprint(\"-\"*80)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73f4107e-db4d-4937-80aa-98c27972be65"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["And of course we can view that data here:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38fd64f5-9ff0-48bd-ab33-87cbfba128e0"}}},{"cell_type":"code","source":["display(jsonDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a487486e-0bcd-4172-87a9-e56c1396e188"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Next steps\n\nStart the next lesson, [Reading Data - Parquet]($./3.Reading%20Data%20-%20Parquet)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1aaccbc0-e8be-4e4e-aa20-2858d5a8ce3d"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2.Reading Data - JSON","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":886737020519654}},"nbformat":4,"nbformat_minor":0}
